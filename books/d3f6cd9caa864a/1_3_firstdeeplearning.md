---
title: "ディープラーニングの基礎 - ディープラーニングの限界"
---

# ゴール

この章のゴールは、ディープラーニングの限界がどこにあるのか、何によって限界が訪れるのかを理解することです。

# 開発環境

:::details 前章と同じです。

[uv](https://zenn.dev/malt03/books/d3f6cd9caa864a/viewer/999_environment#uv) をインストールしてください。

### ディレクトリ

本章では、`./point_classifier_2d` 配下のコードを解説します。  
ディレクトリに移動し、 `sync` を実行してください。

```sh
cd ./point_classifier_2d
uv sync
```

:::

# 実行してみよう

まずは、学習データを表示してみましょう。
この学習データにはどのような法則があるでしょうか？

```sh
uv run ./src/dump_points.py ./data/random.csv
```

ファイル名からもわかる通り、この学習データには何の法則性もありません。
ただランダムな座標に対してランダムに赤と青に分類しているだけです。

このデータを学習した場合、どのような結果になるのか予想してみてください。
損失値は下がり学習は進むでしょうか？

実行してみましょう。

```sh
uv run ./src/train.py ./data/random.csv complex
```

```
Epoch 0, Loss: 0.6933952569961548
Epoch 1000, Loss: 0.46927380561828613
Epoch 2000, Loss: 0.4244226813316345
Epoch 3000, Loss: 0.3807366192340851
Epoch 4000, Loss: 0.3349246084690094
Epoch 5000, Loss: 0.30404770374298096
Epoch 6000, Loss: 0.27357009053230286
Epoch 7000, Loss: 0.25301170349121094
Epoch 8000, Loss: 0.22965030372142792
Epoch 9000, Loss: 0.21256697177886963
Epoch 10000, Loss: 0.2006889283657074
Epoch 11000, Loss: 0.17443828284740448
Epoch 12000, Loss: 0.15796338021755219
Epoch 13000, Loss: 0.14741690456867218
Epoch 14000, Loss: 0.13447317481040955
Epoch 15000, Loss: 0.12287665158510208
Epoch 16000, Loss: 0.11438805609941483
Epoch 17000, Loss: 0.10875195264816284
Epoch 18000, Loss: 0.11644843965768814
Epoch 19000, Loss: 0.09544093906879425
Epoch 20000, Loss: 0.09283038228750229
```

損失値が下がり学習が進行していることがわかります。
完全にランダムなデータに対しても学習することができました。
これはどういうことでしょうか？

# 解説

学習の結果に表示されたグラフと、学習データのグラフを比較してみてください。
モデルがランダムなパターンをそのまま記憶していることがわかるはずです。
ランダムに赤がある部分には赤があると学習し、ランダムに青がある部分には青があると学習しています。

これまで述べてきたように、ディープラーニングではモデルがデータに基づいて境界線を探索します。
この境界線の複雑さには、理論上の限界はありません。層を増やして複雑なモデルを作れば、どんなパターンにも適応することが可能です。

:::message
実際には計算リソースが制約となります。層を増やしてモデルを複雑化していくと、計算コストが増大し、必要なメモリや計算時間が膨大になり、実用上の限界が訪れます。
:::

その結果、ランダムなパターンに対しても、入力（座標）と出力（ラベル）を一対一対応させる形で学習を進めます。
これは意味のあるパターンを学習しているわけではなく、ただデータを暗記している状態です。

ディープラーニングのすごいところは、入力と出力さえあれば、どんなに複雑なデータでも（意味のないデータですら）「学習ができない」という状況がないことです。
この柔軟性により、非常に多くの問題に適用できる技術となっています。
