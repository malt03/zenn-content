---
title: "ディープラーニングの基礎 - 最も単純な2Dポイント分類モデル"
---

# ゴール

この章のゴールは、ディープラーニングがどのようにデータを学習し、分類していくかを、最も単純なモデルを作成しながら理解することです。
画像認識も LLM も、全てのディープラーニングはこのモデルの延長です。

# 開発環境

[uv](https://zenn.dev/malt03/books/d3f6cd9caa864a/viewer/999_environment#uv) をインストールしてください。

### ディレクトリ

本章では `./point_classifier_2d` 配下のコードを解説します。  
ディレクトリに移動し、 `sync` を実行してください。

```sh
cd ./point_classifier_2d
uv sync
```

# 実行してみよう

まずは実行してみましょう。ここでは`train.py`を実行し、2D 平面上でのデータ分類の流れを確認します。

```sh
uv run ./src/train.py ./data/linear.csv simple
```

以下のような出力とグラフが表示されれば、正しく実行できています。  
この時点でうまくいかない場合は、すぐに[筆者に連絡](https://zenn.dev/malt03/books/d3f6cd9caa864a/viewer/0_intro#%E3%83%88%E3%83%A9%E3%83%96%E3%83%AB%E3%82%B7%E3%83%A5%E3%83%BC%E3%83%86%E3%82%A3%E3%83%B3%E3%82%B0)してください。

```
Epoch 0, Loss: 0.8013088703155518
Epoch 1000, Loss: 0.45744192600250244
Epoch 2000, Loss: 0.3413170576095581
Epoch 3000, Loss: 0.27807143330574036
Epoch 4000, Loss: 0.24008016288280487
Epoch 5000, Loss: 0.2156420350074768
Epoch 6000, Loss: 0.19934573769569397
Epoch 7000, Loss: 0.1883869767189026
Epoch 8000, Loss: 0.1811341494321823
Epoch 9000, Loss: 0.1765332669019699
Epoch 10000, Loss: 0.173834890127182
Epoch 11000, Loss: 0.17245401442050934
Epoch 12000, Loss: 0.17189756035804749
Epoch 13000, Loss: 0.17175136506557465
Epoch 14000, Loss: 0.17173343896865845
Epoch 15000, Loss: 0.1717328429222107
Epoch 16000, Loss: 0.1717328429222107
Epoch 17000, Loss: 0.1717328429222107
Epoch 18000, Loss: 0.1717328429222107
Epoch 19000, Loss: 0.1717328429222107
Epoch 20000, Loss: 0.1717328429222107
2024-11-14 23:24:55.320 python3[92135:11524095] +[IMKClient subclass]: chose IMKClient_Legacy
2024-11-14 23:24:55.320 python3[92135:11524095] +[IMKInputSession subclass]: chose IMKInputSession_Legacy
```

![linear](/images/d3f6cd9caa864a/1_1_firstdeeplearning/linear.png)

出力には「損失値（Loss）」と呼ばれる値を表示しています。これはモデルの学習精度を示す指標で、数値が小さいほど学習が進んでいることを意味します。  
`train.py` では、20,000 回（Epoch）の訓練を行っています。
Epoch とは、モデルが全データを一通り学習することを 1 回と数えた単位です。今回は 1,000 回ごとに損失値を表示しています。  
訓練が進むにつれて数値が小さくなり学習が進んでいること、13,000 回程度で学習が収束していることが確認できます。

# 解説

### ディープラーニングとは何か

すごく雑に言ってしまえば、ディープラーニングとは **データの集合に対して境界線を探索する** 技術です。  
上で実行した結果では、データから学習し、2 次元上の座標の集合に対して直線の境界線を探索できています。

### `train.py` が何をしているか

`./data/linear.csv` を開いてください。

```csv:linear.csv
3.949785611484774028e-01,2.102803861232633853e-01,0.000000000000000000e+00
2.536423547222862451e-01,9.542483706055390202e-01,1.000000000000000000e+00
7.231352180317337508e-01,2.749202113120079449e-01,0.000000000000000000e+00
...
```

左から、x 座標、y 座標、分類です。  
0 から 1 の 2D 平面上に、0（赤）と 1（青）に分類されたデータが 200 個あります。  
表示してみましょう。

```sh
uv run ./src/dump_points.py ./data/linear.csv
```

`train.py` では、この平面上で境界線を探索し、作成した境界線を使ってランダムな 1,000 個の座標を分類しています。

# コードを読む

### main 関数

```python:train.py
points, labels = load_data(sys.argv[1])
model = construct_model(sys.argv[2])
train_model(model, points, labels)
random_points, result = exec_for_random_points(model)
```

1 行目で CSV ファイルからデータを読み込み、2 行目でモデルを作成、3 行目でモデルを訓練し、4 行目でランダムな座標に対して分類をしています。
1 行目と 4 行目の細かい実装は本質ではないので、2 行目から見ていきましょう。

### `PointClassifier2DSimple`

`construct_model` 内を見ると、引数に応じてコンストラクタを実行していることが分かります。  
`PointClassifier2DSimple` クラスは、2D ポイントを分類するためのシンプルなニューラルネットワークのモデルです。このクラスの内部を見てみましょう。

```python:train.py
class PointClassifier2DSimple(torch.nn.Module):
    def __init__(self):
        super(PointClassifier2DSimple, self).__init__()
        self.fc1 = torch.nn.Linear(2, 1)

    def forward(self, data):
        data = self.fc1(data)
        data = torch.sigmoid(data)
        return data.squeeze()
```

#### `__init__` メソッド

`__init__`メソッドは、クラスが作られたときに最初に呼び出されるコンストラクタです。  
モデルを定義する際は、ここでモデルの基本となるニューラルネットワークの層を作成します。
層とは、モデルにおける細胞のようなもので、層を積み重ねてモデルを形成します。

`nn.Linear(n, m)` は、n 個の入力を m 個の出力に変換する層で、全結合層と呼ばれます。  
今回は、`(x, y)` の 2 つのデータを入力し、それを分類用の 1 つの出力に変換する層を一つだけ作成しました。

今後様々な層が出てきますが、最も基本的で最もよく使われるのがこの全結合層です。  
新たな層が出てきたときに、その層を理解するには PyTorch の公式リファレンスを見るのが早道ですが、今見ても何もわからないと思うので、ちらっと見ておく程度にしておくのが良いでしょう。
https://pytorch.org/docs/stable/generated/torch.nn.Linear.html

#### `forward` メソッド

`forward` メソッドでは、データを入力したときにどのように処理すかを定義します。  
実際にデータを入力すると、このメソッドを使って計算が行われ、出力が得られます。

`data = self.fc1(data)`
データがまず全結合層 `fc1` を通ります。  
この層で、x と y の 2 つの値が、全結合層インスタンスに保管された「重み」や「バイアス」と呼ばれるプロパティを使って 1 つの値に変換されます。  
「重み」と「バイアス」が学習の過程で最適化されることで、データの境界線を探索します。

`data = torch.sigmoid(data)`
その後、sigmoid という関数を使って、出力を 0 から 1 の間に変換します。  
こういった、層とは違ってプロパティを持たず、値を変換する純粋関数を「活性化関数」と呼びます。

活性化関数はニューラルネットワークに「柔軟な表現力」を与える重要な要素で、層と同様にたくさんの種類があります。
どんな活性化関数か知りたい場合は Google で画像検索してグラフを見るとイメージを掴みやすいです。  
数式を見ても頭が痛くなるだけで何のことやら分からないので、研究者になりたい場合以外は見ないようにしましょう。  
https://www.google.com/search?q=sigmoid+activation+function&udm=2

sigmoid 関数は、どんな入力も 0 以上 1 以下の値になだらかに変換する特徴があり、二値分類問題ではほぼ必ず使われます。  
「二値分類問題」とは、データを「2 つの値のどちらか」に分類する問題で、最も基本的な分類問題です。  
今回も座標を赤と青の 2 つに分類しているので二値分類問題です。

`return data.squeeze()`
`squeeze`は、次元を操作する関数です。  
ここでは、学習データと形式を合わせるために使用していますが、処理そのものには影響せず、あまり重要ではありません。

#### `PointClassifier2DSimple` まとめ

このクラスは、シンプルな全結合層と sigmoid 関数を使って、2D データの二値分類を行うモデルで、分類結果が 0 以上 1 以下で返されます。  
この値は「一方の分類である確率」を示しており、今回で言えば「青である確率」を表しています。

実務上は、出力が 0.5 未満であれば A、0.5 以上であれば B といった形で、確率を基に最終的な分類を行います。

### `train_model`

どのようにモデルを定義するか理解したところで、次にモデルの訓練を見ていきましょう。  
この関数では、モデルのインスタンス（model）、ポイントデータ（points）と、それぞれのデータがどの分類に属するかを示すラベル（labels）を引数に取り、モデルのインスタンスを学習させます。

```python:train.py
def train_model(model, points, labels):
    loss_fn = torch.nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0003)

    for epoch in range(20001):
        optimizer.zero_grad()
        outputs = model(points)
        loss = loss_fn(outputs, labels)
        loss.backward()
        optimizer.step()

        if epoch % 1000 == 0:
            print(f"Epoch {epoch}, Loss: {loss.item()}")
```

#### `loss_fn = torch.nn.BCELoss()`

ここでは「損失関数」と呼ばれるものを初期化しています。  
損失関数とは、**モデルの出力と期待される正解値との差を計算するための関数**で、ディープラーニングの学習において中心的な役割を果たします。  
損失関数の戻り値（損失）が小さいほど、差が小さくなっていることを意味します。

損失関数も様々な種類が存在しますが、よく使われるものは限られています。

|       名前       | 適した問題               | 予測する値の例                 |
| :--------------: | :----------------------- | :----------------------------- |
|     BCELoss      | 二値分類問題             | スパムか否か、画像内の人の有無 |
| CrossEntropyLoss | 多値分類問題             | 手書き数字、テキストの言語     |
|     MSELoss      | 回帰問題（連続値の予測） | 株価、気温                     |

今回は二値分類問題ですから、`BCELoss` を使っています。

損失関数はほぼこの 3 つとその派生で事足ります。LLM の学習でも `CrossEntropyLoss` が使われています。

#### `optimizer = torch.optim.Adam(model.parameters(), lr=0.003)`

ここでは「オプティマイザ」と呼ばれるものを設定しています。  
損失値に応じて、重みやバイアスをどのような速度で調整するかを決定するアルゴリズム、と理解しておけば間違いではないでしょう。

オプティマイザにも様々な種類がありますが、とりあえず `Adam` を使っておけば大抵うまくいきます。逆に、学習がうまくいかないからといってオプティマイザを変更してもうまくいかないことが多いです。

重要なパラメータとして学習率（lr）があり、学習の速度を調整できます。  
値が大きいと学習が不安定になり、小さいと学習が遅くなります。  
学習を何度か走らせてみて調整しましょう。

#### for ループ

for ループの中身は学習におけるお決まりのコードなので、あまり深く考えず何となくの理解で大丈夫です。

- `optimizer.zero_grad()`
  - 学習開始のおまじない。
- `outputs = model(points)`
  - モデルにデータを入力し、出力を取得します。これは今の時点のモデルがデータに基づいて出した分類の予測です。
- `loss = loss_fn(outputs, labels)`
  - モデルの出力と期待される正解値から損失を計算します。この値が小さいほど、モデルの出力が正解値に近いことを示します。
- `loss.backward()`
  - 損失に基づいて学習の方向を計算します。「最適な調整方向を計算できる」ことがディープラーニングの革命的なところです。
- `optimizer.step()`
  - 学習の方向に向けてモデルのパラメータを更新します。これを繰り返すことで、モデルがデータに適応していきます。

# まとめ

ここまでで、ディープラーニングにおける重要な概念について全て触れたと言っても過言ではありません。
以下のキーワードの理解を深め、自在に実装できれば、ディープラーニングマスターです。
何となくでも自分の言葉で説明できるよう、振り返っておきましょう。

- 層
- 損失関数
- 活性化関数
- オプティマイザ
